{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bd8ccd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import math\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03642f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1.0, 2.0])       # Features\n",
    "y_train = np.array([300.0, 500.0])   # Targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8853638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, Y, w, b):\n",
    "    return np.mean((np.dot(X, w) + b - Y) ** 2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0d5c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, w, b):\n",
    "    dj_dw = np.mean((np.dot(X, w) + b - Y) * X)\n",
    "    dj_db = np.mean(np.dot(X, w) + b - Y)\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f8baafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_line(dj_dx, x1, y1, d, ax):\n",
    "    x = np.linspace(x1-d, x1+d, 50)\n",
    "    y = dj_dx * (x - x1) + y1\n",
    "\n",
    "    ax.scatter(x1, y1, color='blue', s=50)\n",
    "    ax.plot(x, y, '--', lw=1, color='darkred', zorder=10)\n",
    "    xoff = 30 if x1 == 200 else 10\n",
    "    ax.annotate(r'$\\frac{\\partial J}{\\partial w} =%d$' % dj_dx, xy=(x1, y1), xycoords='data',\n",
    "                xytext=(xoff, 10), textcoords='offset points')\n",
    "    \n",
    "    \n",
    "def plt_gradients(X, Y, f_compute_cost, f_compute_gradient):\n",
    "    fixed_b = 100\n",
    "    w_array = np.linspace(0, 400, 50)\n",
    "    cost = np.zeros_like(w_array)\n",
    "\n",
    "    for i in range(len(w_array)):\n",
    "        cost[i] = f_compute_cost(X, Y, w_array[i], fixed_b)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    ax.plot(w_array, cost, lw=1, color='blue')\n",
    "    ax.set_ylabel('Cost')\n",
    "    ax.set_xlabel('w')    \n",
    "    ax.set_title('Cost vs w, with gradient; b set to 100')\n",
    "\n",
    "    for curr_w in [100, 200, 300]:\n",
    "        dj_dw, dj_db = f_compute_gradient(X, Y, curr_w, fixed_b)\n",
    "        cost_at_w = f_compute_cost(X, Y, curr_w, fixed_b)\n",
    "        make_line(dj_dw, curr_w, cost_at_w, 30, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54502c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_gradients(x_train,y_train, compute_cost, compute_gradient)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e9900da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    w, b = w_in, b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        dj_dw, dj_db = gradient_function(X, Y, w, b)\n",
    "        w -= alpha * dj_dw\n",
    "        b -= alpha * dj_db\n",
    "\n",
    "        if i < 100000:      # prevent resource exhaustion \n",
    "            J_history.append(cost_function(X, Y, w,b))\n",
    "            p_history.append([w, b])\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or num_iters - i <= 5:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "            \n",
    "        if len(J_history) > 1 and abs(J_history[-2] - J_history[-1]) < 1e-6:\n",
    "            break\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba09c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init, b_init, alpha, num_iters = 0, 0, 1.0e-2, 10000\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train, y_train, w_init, b_init, alpha, num_iters, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966366a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\n",
    "ax1.set_title(\"Cost vs. iteration(start)\"); ax2.set_title(\"Cost vs. iteration(end)\")\n",
    "ax1.set_ylabel('Cost'); ax2.set_ylabel('Cost')\n",
    "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f378418",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "49e6f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inbounds(point, base, xlim, ylim):\n",
    "    arr = np.array([point, base])  # shape (2,2)\n",
    "    return np.all((xlim[0] <= arr[:,0]) & (arr[:,0] <= xlim[1]) &\n",
    "                  (ylim[0] <= arr[:,1]) & (arr[:,1] <= ylim[1]))\n",
    "\n",
    "\n",
    "def plt_contour_wgrad(X, Y, hist, ax, w_range=[-100, 500, 5], b_range = [-500, 500, 5], w_final=200, b_final=100, resolution=5, \n",
    "                      contours = [0.1,50,1000,5000,10000,25000,50000],\n",
    "                      steps=10):\n",
    "    b0, w0 = np.meshgrid(np.arange(*b_range), np.arange(*w_range))\n",
    "    z = np.zeros_like(b0)\n",
    "    for i in range(w0.shape[0]):\n",
    "        for j in range(w0.shape[1]):\n",
    "            z[i][j] = compute_cost(X, Y, w0[i][j], b0[i][j])\n",
    "            if z[i][j] == 0:\n",
    "                z[i][j] = 1e-6\n",
    "                \n",
    "\n",
    "    CS = ax.contour(w0, b0, z, contours, linewidths=2)\n",
    "    ax.clabel(CS, inline=1, fmt='%1.0f', fontsize=10)\n",
    "    ax.set_xlabel(\"w\"); ax.set_ylabel(\"b\")\n",
    "    ax.set_title('Contour plot of cost J(w,b), vs b,w with path of gradient descent')\n",
    "    w = w_final; b = b_final\n",
    "    ax.hlines(b, ax.get_xlim()[0], w, lw=2, color='purple', ls='dotted')\n",
    "    ax.vlines(w, ax.get_ylim()[0], b, lw=2, color='purple', ls='dotted')\n",
    "\n",
    "    base = hist[0]\n",
    "    for point in hist[0::steps]:\n",
    "        edist = math.sqrt((point[0] - base[0])**2 + (point[1] - base[1])**2)\n",
    "        if (edist > resolution or point==hist[-1]):\n",
    "            if inbounds(point, base, ax.get_xlim(), ax.get_ylim()):\n",
    "                plt.annotate('', xy=point, xytext=base, xycoords='data',\n",
    "                             arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 3},\n",
    "                         va='center', ha='center')\n",
    "            base=point\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037214f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 4))\n",
    "plt_contour_wgrad(x_train, y_train, p_hist, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5911e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 4))\n",
    "plt_contour_wgrad(x_train, y_train, p_hist, ax, w_range=[180, 220, 0.5], b_range=[80, 120, 0.5],\n",
    "            contours=[1,5,10,20],resolution=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f24d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# set alpha to a large value\n",
    "iterations = 10\n",
    "tmp_alpha = 8.0e-1\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ec547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_divergence(p_hist, J_hist, X, Y):\n",
    "    x = np.zeros(len(p_hist))\n",
    "    y = np.zeros(len(p_hist))\n",
    "    v = np.zeros(len(p_hist))\n",
    "    for i in range(len(p_hist)):\n",
    "        x[i] = p_hist[i][0]\n",
    "        y[i] = p_hist[i][1]\n",
    "        v[i] = J_hist[i]\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    plt.subplots_adjust(wspace=0)\n",
    "    gs = fig.add_gridspec(1, 5)\n",
    "    fig.suptitle(f\"Cost escalates when learning rate is too large\")\n",
    "    ax = fig.add_subplot(gs[:2])\n",
    "    fixed_b = 100\n",
    "    w_array = np.arange(-70000, 70000, 1000)\n",
    "    cost = np.zeros_like(w_array)\n",
    "\n",
    "    for i in range(len(w_array)):\n",
    "        cost[i] = compute_cost(X, Y, w_array[i], fixed_b)\n",
    "    \n",
    "    ax.plot(w_array, cost)\n",
    "    ax.plot(x, v, color='magenta')\n",
    "    ax.set_title(\"Cost vs w, b set to 100\")\n",
    "    ax.set_ylabel('Cost')\n",
    "    ax.set_xlabel('w')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(2))\n",
    "\n",
    "    tmp_b, tmp_w = np.meshgrid(np.arange(-35000, 35000, 500), np.arange(-70000, 70000, 500))\n",
    "    z = np.zeros_like(tmp_w)\n",
    "    for i in range(tmp_w.shape[0]):\n",
    "        for j in range(tmp_w.shape[1]):\n",
    "            z[i][j] = compute_cost(X, Y, tmp_w[i][j], tmp_b[i][j])\n",
    "\n",
    "    ax = fig.add_subplot(gs[2:], projection='3d')\n",
    "    ax.plot_surface(tmp_w, tmp_b, z, alpha=0.3, color='blue')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(2))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(2))\n",
    "    ax.set_xlabel('w', fontsize=16)\n",
    "    ax.set_ylabel('b', fontsize=16)\n",
    "    ax.set_zlabel('\\ncost', fontsize=16)\n",
    "    ax.set_title('Cost vs (b, w)')\n",
    "    ax.view_init(elev=20., azim=-65)\n",
    "    ax.plot(x, y, v, color='magenta')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54329880",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_divergence(p_hist, J_hist, x_train, y_train)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-specialization-playground-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
