{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2b2b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573cb7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[5, 8], [1, 1], [10, 9], [3, 2], [6, 5], [7, 8]], dtype=np.float64)\n",
    "y_train = np.array([1.0, 0, 1, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69310195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.asarray(z, dtype=np.float64)\n",
    "    out = np.where(z >= 0, 1 / (1 + np.exp(-z)), np.exp(z) / (1 + np.exp(z)))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4669e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_1pexp(z):\n",
    "    z = np.asarray(z, dtype=np.float64)\n",
    "    out = np.where(z >= 0, z + np.log1p(np.exp(-z)), np.log1p(np.exp(z)))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa89bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b, lambda_=0):\n",
    "    m = X.shape[0]\n",
    "    z = np.dot(X, w) + b\n",
    "    cost = -y * z + log_1pexp(z)\n",
    "    cost = np.sum(cost) + (lambda_ / 2) * np.sum(w**2)\n",
    "    cost = cost / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16d99c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, w, b, lambda_=0):\n",
    "    m = X.shape[0]\n",
    "    z = np.dot(X, w) + b\n",
    "    f_wb = sigmoid(z)\n",
    "    e = f_wb - y\n",
    "    dj_db = (1 / m) * np.sum(e)\n",
    "    dj_dw = (1 / m) * (np.dot(X.T, e) + lambda_ * w)\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c1ec605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters, lambda_=0):\n",
    "    J_history = []\n",
    "    params_history = []\n",
    "    grads_history = []\n",
    "    iter_history = []\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "    lambda_ = lambda_\n",
    "    alpha = alpha\n",
    "    save_interval = math.ceil(num_iters / 100000)\n",
    "    cost_tol = 1e-6\n",
    "    prev_cost = None\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        dj_db, dj_dw = compute_gradients(X, y, w, b, lambda_)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        cost = compute_cost(X, y, w, b, lambda_)\n",
    "        if i == 0 or i % save_interval == 0:\n",
    "            J_history.append(cost)\n",
    "            params_history.append([w, b])\n",
    "            grads_history.append([dj_dw, dj_db])\n",
    "            iter_history.append(i)\n",
    "\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(\n",
    "                f\"Iter {i}: Cost = {cost:.5f}, w = {w}, b = {b:.5f}, dj_dw = {dj_dw}, dj_db = {dj_db:.5f}\"\n",
    "            )\n",
    "            # Early stopping\n",
    "        if prev_cost is not None and abs(cost - prev_cost) < cost_tol:\n",
    "            print(\n",
    "                f\"Early stopping at iter {i} — Δloss = {abs(cost - prev_cost):.2e} < {cost_tol}\"\n",
    "            )\n",
    "            break\n",
    "        prev_cost = cost\n",
    "    print(\n",
    "        f\"Final w : {w}, Final b : {b:.5f}, cost : {cost:.5f}, dj_dw = {dj_dw}, dj_db = {dj_db:.5f}\"\n",
    "    )\n",
    "    return w, b, J_history, params_history, grads_history, iter_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7672c143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Cost = 0.66586, w = [0.01       0.01416667], b = 0.00000, dj_dw = [-1.         -1.41666667], dj_db = 0.00000\n",
      "Final w : [0.01       0.01416667], Final b : 0.00000, cost : 0.66586, dj_dw = [-1.         -1.41666667], dj_db = 0.00000\n"
     ]
    }
   ],
   "source": [
    "w_in = np.zeros_like(X_train[0])\n",
    "b_in = 0.0\n",
    "alpha = 0.01\n",
    "lambda_ = 0.5\n",
    "w_out, b_out, j_hist, params_hist, grads_hist, iter_hist = gradient_descent(\n",
    "    X_train, y_train, w_in, b_in, alpha, num_iters=1, lambda_=lambda_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e815ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Cost = 0.66586, w = [0.01       0.01416667], b = 0.00000, dj_dw = [-1.         -1.41666667], dj_db = 0.00000\n",
      "Iter 1000: Cost = 0.32521, w = [-0.46960778  0.79615553], b = -1.26930, dj_dw = [ 0.0078412  -0.02252826], dj_db = 0.09579\n",
      "Iter 2000: Cost = 0.26119, w = [-0.46318001  0.90631992], b = -2.05470, dj_dw = [-0.0046583  -0.00477104], dj_db = 0.06516\n",
      "Iter 3000: Cost = 0.22872, w = [-0.41163478  0.93587007], b = -2.61958, dj_dw = [-0.00509238 -0.00195387], dj_db = 0.04931\n",
      "Iter 4000: Cost = 0.20891, w = [-0.3651383   0.95242745], b = -3.06102, dj_dw = [-0.00418736 -0.00148387], dj_db = 0.03969\n",
      "Iter 5000: Cost = 0.19556, w = [-0.32740739  0.96656876], b = -3.42373, dj_dw = [-0.00339897 -0.00136029], dj_db = 0.03324\n",
      "Iter 6000: Cost = 0.18594, w = [-0.29642254  0.97973932], b = -3.73181, dj_dw = [-0.00283056 -0.00127493], dj_db = 0.02861\n",
      "Iter 7000: Cost = 0.17867, w = [-0.270276    0.99207197], b = -3.99970, dj_dw = [-0.00242032 -0.00119193], dj_db = 0.02512\n",
      "Iter 8000: Cost = 0.17298, w = [-0.24767596  1.0035873 ], b = -4.23670, dj_dw = [-0.00211391 -0.00111197], dj_db = 0.02239\n",
      "Iter 9000: Cost = 0.16841, w = [-0.2277711   1.01432899], b = -4.44918, dj_dw = [-0.00187676 -0.00103747], dj_db = 0.02018\n",
      "Final w : [-0.21000065  1.0243475 ], Final b : -4.64147, cost : 0.16466, dj_dw = [-0.00168775 -0.00096938], dj_db = 0.01837\n"
     ]
    }
   ],
   "source": [
    "w_in = np.zeros_like(X_train[0])\n",
    "b_in = 0.0\n",
    "alpha = 0.01\n",
    "lambda_ = 0.5\n",
    "num_iters = 10000\n",
    "w_out, b_out, j_hist, params_hist, grads_hist, iter_hist = gradient_descent(\n",
    "    X_train, y_train, w_in, b_in, alpha, num_iters=num_iters, lambda_=lambda_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28f93c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Cost = 0.66585, w = [0.01       0.01416667], b = 0.00000, dj_dw = [-1.         -1.41666667], dj_db = 0.00000\n",
      "Iter 1000: Cost = 0.26038, w = [-0.71952331  1.06323864], b = -1.26324, dj_dw = [ 0.03947961 -0.05677925], dj_db = 0.09370\n",
      "Iter 2000: Cost = 0.17446, w = [-1.00898059  1.49383776], b = -2.02172, dj_dw = [ 0.02200468 -0.03360902], dj_db = 0.06215\n",
      "Iter 3000: Cost = 0.13409, w = [-1.19308914  1.7778563 ], b = -2.55661, dj_dw = [ 0.01563386 -0.02430232], dj_db = 0.04639\n",
      "Iter 4000: Cost = 0.11031, w = [-1.33138765  1.9935251 ], b = -2.97042, dj_dw = [ 0.01232865 -0.01927035], dj_db = 0.03710\n",
      "Iter 5000: Cost = 0.09445, w = [-1.44374437  2.16929026], b = -3.30897, dj_dw = [ 0.01028708 -0.01609983], dj_db = 0.03100\n",
      "Iter 6000: Cost = 0.08303, w = [-1.53922345  2.31870505], b = -3.59633, dj_dw = [ 0.00888897 -0.01390644], dj_db = 0.02670\n",
      "Iter 7000: Cost = 0.07435, w = [-1.62274347  2.44930601], b = -3.84659, dj_dw = [ 0.00786443 -0.01229068], dj_db = 0.02350\n",
      "Iter 8000: Cost = 0.06750, w = [-1.69728729  2.565731  ], b = -4.06870, dj_dw = [ 0.00707704 -0.01104569], dj_db = 0.02102\n",
      "Iter 9000: Cost = 0.06194, w = [-1.7648094  2.6710466], b = -4.26868, dj_dw = [ 0.00645025 -0.01005354], dj_db = 0.01905\n",
      "Final w : [-1.82660609  2.76729958], Final b : -4.45061, cost : 0.05731, dj_dw = [ 0.00593811 -0.00924271], dj_db = 0.01743\n"
     ]
    }
   ],
   "source": [
    "w_in = np.zeros_like(X_train[0])\n",
    "b_in = 0.0\n",
    "alpha = 0.01\n",
    "lambda_ = 0\n",
    "num_iters = 10000\n",
    "w_out, b_out, j_hist, params_hist, grads_hist, iter_hist = gradient_descent(\n",
    "    X_train, y_train, w_in, b_in, alpha, num_iters=num_iters, lambda_=lambda_\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-specialization-playground-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
