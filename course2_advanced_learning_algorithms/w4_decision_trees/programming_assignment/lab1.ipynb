{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from public_tests import *\n",
    "import utils_practice as utils\n",
    "from utils import generate_split_viz, generate_tree_viz\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f391401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(\n",
    "    [\n",
    "        [1, 1, 1],\n",
    "        [1, 0, 1],\n",
    "        [1, 0, 0],\n",
    "        [1, 0, 0],\n",
    "        [1, 1, 1],\n",
    "        [0, 1, 1],\n",
    "        [0, 0, 0],\n",
    "        [1, 0, 1],\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 0],\n",
    "    ]\n",
    ")\n",
    "y_train = np.array([1, 1, 0, 0, 1, 0, 0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88392af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First few elements of X_train:\\n {X_train[:5]}\")\n",
    "print(f\"Type of X_train: {type(X_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3922af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First few elements of y_train: {y_train[:5]}\")\n",
    "print(f\"Type of y_train: {type(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd690b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of X_train is:\", X_train.shape)\n",
    "print(\"The shape of y_train is: \", y_train.shape)\n",
    "print(\"Number of training examples (m):\", len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75faeb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: compute_entropy\n",
    "\n",
    "\n",
    "def compute_entropy(y):\n",
    "    \"\"\"\n",
    "    Computes the entropy for\n",
    "\n",
    "    Args:\n",
    "       y (ndarray): Numpy array indicating whether each example at a node is\n",
    "           edible (`1`) or poisonous (`0`)\n",
    "\n",
    "    Returns:\n",
    "        entropy (float): Entropy at that node\n",
    "\n",
    "    \"\"\"\n",
    "    # You need to return the following variables correctly\n",
    "    entropy = 0.0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "\n",
    "    label_is_1 = y == 1\n",
    "    p = np.mean(label_is_1)\n",
    "    if p == 0 or p == 1:\n",
    "        return 0\n",
    "    entropy = -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute entropy at the root node (i.e. with all examples)\n",
    "# Since we have 5 edible and 5 non-edible mushrooms, the entropy should be 1\"\n",
    "\n",
    "print(\"Entropy at root node: \", compute_entropy(y_train))\n",
    "\n",
    "# UNIT TESTS\n",
    "compute_entropy_test(compute_entropy)\n",
    "compute_entropy(np.array([0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f84d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: split_dataset\n",
    "\n",
    "\n",
    "def split_dataset(X, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Splits the data at the given node into\n",
    "    left and right branches\n",
    "\n",
    "    Args:\n",
    "        X (ndarray):             Data matrix of shape(n_samples, n_features)\n",
    "        node_indices (list):     List containing the active indices. I.e, the samples being considered at this step.\n",
    "        feature (int):           Index of feature to split on\n",
    "\n",
    "    Returns:\n",
    "        left_indices (list):     Indices with feature value == 1\n",
    "        right_indices (list):    Indices with feature value == 0\n",
    "    \"\"\"\n",
    "\n",
    "    # You need to return the following variables correctly\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    node_indices = np.asarray(node_indices)\n",
    "    ones = X[node_indices, feature] == 1\n",
    "    left_indices = node_indices[ones].tolist()\n",
    "    right_indices = node_indices[~ones].tolist()\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77747669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1\n",
    "\n",
    "root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Feel free to play around with these variables\n",
    "# The dataset only has three features, so this value can be 0 (Brown Cap), 1 (Tapering Stalk Shape) or 2 (Solitary)\n",
    "feature = 0\n",
    "\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices, feature)\n",
    "\n",
    "print(\"CASE 1:\")\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)\n",
    "\n",
    "# Visualize the split\n",
    "generate_split_viz(root_indices, left_indices, right_indices, feature)\n",
    "\n",
    "print()\n",
    "\n",
    "# Case 2\n",
    "\n",
    "root_indices_subset = [0, 2, 4, 6, 8]\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices_subset, feature)\n",
    "\n",
    "print(\"CASE 2:\")\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)\n",
    "\n",
    "# Visualize the split\n",
    "generate_split_viz(root_indices_subset, left_indices, right_indices, feature)\n",
    "\n",
    "# UNIT TESTS\n",
    "split_dataset_test(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: compute_information_gain\n",
    "\n",
    "\n",
    "def compute_information_gain(X, y, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Compute the information of splitting the node on a given feature\n",
    "\n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "        feature (int):           Index of feature to split on\n",
    "\n",
    "    Returns:\n",
    "        cost (float):        Cost computed\n",
    "\n",
    "    \"\"\"\n",
    "    # Split dataset\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "\n",
    "    # Some useful variables\n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "\n",
    "    # You need to return the following variables correctly\n",
    "    information_gain = 0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    h_node = compute_entropy(y_node)\n",
    "    h_left, h_right = compute_entropy(y_left), compute_entropy(y_right)\n",
    "    w_left, w_right = len(y_left) / len(y_node), len(y_right) / len(y_node)\n",
    "    information_gain = h_node - (w_left * h_left + w_right * h_right)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91eec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain0 = compute_information_gain(X_train, y_train, root_indices, feature=0)\n",
    "print(\"Information Gain from splitting the root on brown cap: \", info_gain0)\n",
    "\n",
    "info_gain1 = compute_information_gain(X_train, y_train, root_indices, feature=1)\n",
    "print(\"Information Gain from splitting the root on tapering stalk shape: \", info_gain1)\n",
    "\n",
    "info_gain2 = compute_information_gain(X_train, y_train, root_indices, feature=2)\n",
    "print(\"Information Gain from splitting the root on solitary: \", info_gain2)\n",
    "\n",
    "# UNIT TESTS\n",
    "compute_information_gain_test(compute_information_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c5e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION: get_best_split\n",
    "\n",
    "\n",
    "def get_best_split(X, y, node_indices):\n",
    "    \"\"\"\n",
    "    Returns the optimal feature and threshold value\n",
    "    to split the node data\n",
    "\n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "\n",
    "    Returns:\n",
    "        best_feature (int):     The index of the best feature to split\n",
    "    \"\"\"\n",
    "\n",
    "    # Some useful variables\n",
    "    num_features = X.shape[1]\n",
    "\n",
    "    # You need to return the following variables correctly\n",
    "    best_feature = -1\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    most_info_gain = 0\n",
    "    for feature_idx in range(X.shape[1]):\n",
    "        info_gain = compute_information_gain(X, y, node_indices, feature_idx)\n",
    "        if info_gain > most_info_gain:\n",
    "            best_feature = feature_idx\n",
    "            most_info_gain = info_gain\n",
    "\n",
    "    ### END CODE HERE ##\n",
    "\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc470832",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature = get_best_split(X_train, y_train, root_indices)\n",
    "print(\"Best feature to split on: %d\" % best_feature)\n",
    "\n",
    "# UNIT TESTS\n",
    "get_best_split_test(get_best_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8ea1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = []\n",
    "\n",
    "\n",
    "def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):\n",
    "    \"\"\"\n",
    "    Build a tree using the recursive algorithm that split the dataset into 2 subgroups at each node.\n",
    "    This function just prints the tree.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "        branch_name (string):   Name of the branch. ['Root', 'Left', 'Right']\n",
    "        max_depth (int):        Max depth of the resulting tree.\n",
    "        current_depth (int):    Current depth. Parameter used during recursive call.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Maximum depth reached - stop splitting\n",
    "    if current_depth == max_depth:\n",
    "        format_str = \" \" * current_depth + \"-\" * current_depth\n",
    "        print(f\"{format_str} {branch_name} leaf node with indices {node_indices}\")\n",
    "        return\n",
    "\n",
    "    # Otherwise, get best split and split the data\n",
    "    # Get the best feature and threshold at this node\n",
    "\n",
    "    best_feature = get_best_split(X, y, node_indices)\n",
    "    format_str = \"-\" * current_depth\n",
    "    print(\n",
    "        f\"{format_str} Depth {current_depth}, {branch_name}: Split on feature: {best_feature}\"\n",
    "    )\n",
    "\n",
    "    # Split the dataset at the best feature\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
    "    tree.append((left_indices, right_indices, best_feature))\n",
    "\n",
    "    # continue splitting the left and the right child. Increment current depth\n",
    "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth + 1)\n",
    "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eece29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_tree_recursive(\n",
    "    X_train, y_train, root_indices, \"Root\", max_depth=2, current_depth=0\n",
    ")\n",
    "generate_tree_viz(root_indices, y_train, tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-specialization-playground-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
