{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04949bef",
   "metadata": {},
   "source": [
    "##### Lab Objectives - In this lab, we will:\n",
    "\n",
    "        1. Work with a dataset related to cardiovascular disease\n",
    "        2. Build three different models to estimate how likely a person is to develop cardiovascular disease\n",
    "        3. Implement a Decision Tree model from scikit-learn\n",
    "        4. Implement a Random Forrest model from scikit-learn\n",
    "        5. Implement the XGBoost using its own library\n",
    "        6. Investigate how different parameters on the three models impact their performance\n",
    "\n",
    "        In this notebook, we will:\n",
    "\n",
    "            -Use Pandas to perform one-hot encoding of a dataset\n",
    "            -Use scikit-learn to implement a Decision Tree, Random Forest and XGBoost models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fbc469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"./deeplearning.mplstyle\")\n",
    "\n",
    "RANDOM_STATE = 55  ## We will pass it to every sklearn call so we ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30113fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_variables = [\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c0ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(data=df, prefix=cat_variables, columns=cat_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d010207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    x for x in df.columns if x not in \"HeartDisease\"\n",
    "]  ## Removing our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e10eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f759442",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe5b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df[features], df[\"HeartDisease\"], train_size=0.8, random_state=RANDOM_STATE\n",
    ")\n",
    "# We will keep the shuffle = True since our dataset has not any time dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dea8d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train samples: {len(X_train)}\")\n",
    "print(f\"validation samples: {len(X_val)}\")\n",
    "print(f\"target proportion: {sum(y_train)/len(y_train):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf891f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hyperparameters we will use and investigate here are:\n",
    "\n",
    "# min_samples_split: The minimum number of samples required to split an internal node.\n",
    "# Choosing a higher min_samples_split can reduce the number of splits and may help to reduce overfitting.\n",
    "# max_depth: The maximum depth of the tree.\n",
    "# Choosing a lower max_depth can reduce the number of splits and may help to reduce overfitting.\n",
    "min_samples_split_list = [\n",
    "    2,\n",
    "    10,\n",
    "    30,\n",
    "    50,\n",
    "    100,\n",
    "    200,\n",
    "    300,\n",
    "    700,\n",
    "]  ## If the number is an integer, then it is the actual quantity of samples,\n",
    "max_depth_list = [\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    8,\n",
    "    16,\n",
    "    32,\n",
    "    64,\n",
    "    None,\n",
    "]  # None means that there is no depth limit. Default = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d68e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_train = []\n",
    "accuracy_list_val = []\n",
    "for min_samples_split in min_samples_split_list:\n",
    "    # We can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n",
    "    model = DecisionTreeClassifier(\n",
    "        min_samples_split=min_samples_split, random_state=RANDOM_STATE\n",
    "    ).fit(X_train, y_train)\n",
    "    predictions_train = model.predict(X_train)\n",
    "    predictions_val = model.predict(X_val)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred=predictions_train)\n",
    "    accuracy_val = accuracy_score(y_val, predictions_val)\n",
    "    accuracy_list_train.append(accuracy_train)\n",
    "    accuracy_list_val.append(accuracy_val)\n",
    "\n",
    "plt.title(\"Train x Validation metrics\")\n",
    "plt.xlabel(\"min_samples_split\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xticks(ticks=range(len(min_samples_split_list)), labels=min_samples_split_list)\n",
    "plt.plot(accuracy_list_train)\n",
    "plt.plot(accuracy_list_val)\n",
    "plt.legend([\"Train\", \"Validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a25589",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_train = []\n",
    "accuracy_list_val = []\n",
    "for max_depth in max_depth_list:\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth, random_state=RANDOM_STATE).fit(\n",
    "        X_train, y_train\n",
    "    )\n",
    "    predictions_train = model.predict(X_train)\n",
    "    predictions_val = model.predict(X_val)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred=predictions_train)\n",
    "    accuracy_val = accuracy_score(y_val, predictions_val)\n",
    "    accuracy_list_train.append(accuracy_train)\n",
    "    accuracy_list_val.append(accuracy_val)\n",
    "\n",
    "plt.title(\"Train x Validation metrics\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xticks(ticks=range(len(max_depth_list)), labels=max_depth_list)\n",
    "plt.plot(accuracy_list_train)\n",
    "plt.plot(accuracy_list_val)\n",
    "plt.legend([\"Train\", \"Validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that in general, reducing max_depth can help to reduce overfitting.\n",
    "\n",
    "# Reducing max_depth from 8 to 4 increases validation accuracy closer to training accuracy, while significantly reducing training accuracy.\n",
    "# The validation accuracy reaches the highest at tree_depth=4.\n",
    "# When the max_depth is smaller than 3, both training and validation accuracy decreases. The tree cannot make enough splits to distinguish positives from negatives (the model is underfitting the training set).\n",
    "# When the max_depth is too high ( >= 5), validation accuracy decreases while training accuracy increases, indicating that the model is overfitting to the training set.\n",
    "# So we can choose the best values for these two hyper-parameters for our model to be:\n",
    "\n",
    "# max_depth = 4\n",
    "# min_samples_split = 50\n",
    "decision_tree_model = DecisionTreeClassifier(\n",
    "    min_samples_split=50, max_depth=4, random_state=RANDOM_STATE\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(y_train, decision_tree_model.predict(X_train)):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Metrics validation:\\n\\tAccuracy score: {accuracy_score(y_val, decision_tree_model.predict(X_val)):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98cd864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# n_estimators = the number of Decision Trees that make up the Random Forest. Default is 100\n",
    "# max_features = number of random features to be used at a node in information gain comparison\n",
    "# n_jobs = number of cpu cores to use for training the trees in parallel.\n",
    "min_samples_split_list = [\n",
    "    2,\n",
    "    10,\n",
    "    30,\n",
    "    50,\n",
    "    100,\n",
    "    200,\n",
    "    300,\n",
    "    700,\n",
    "]  ## If the number is an integer, then it is the actual quantity of samples,\n",
    "## If it is a float, then it is the percentage of the dataset\n",
    "max_depth_list = [2, 4, 8, 16, 32, 64, None]\n",
    "n_estimators_list = [10, 50, 100, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_train = []\n",
    "accuracy_list_val = []\n",
    "for min_samples_split in min_samples_split_list:\n",
    "    # We can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n",
    "    model = RandomForestClassifier(\n",
    "        min_samples_split=min_samples_split, random_state=RANDOM_STATE\n",
    "    ).fit(X_train, y_train)\n",
    "    predictions_train = model.predict(X_train)\n",
    "    predictions_val = model.predict(X_val)\n",
    "    accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "    accuracy_val = accuracy_score(y_val, predictions_val)\n",
    "    accuracy_list_train.append(accuracy_train)\n",
    "    accuracy_list_val.append(accuracy_val)\n",
    "\n",
    "plt.title(\"Train x Validation metrics\")\n",
    "plt.xlabel(\"min_samples_split\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xticks(ticks=range(len(min_samples_split_list)), labels=min_samples_split_list)\n",
    "plt.plot(accuracy_list_train)\n",
    "plt.plot(accuracy_list_val)\n",
    "plt.legend([\"Train\", \"Validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f480da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_train = []\n",
    "accuracy_list_val = []\n",
    "for max_depth in max_depth_list:\n",
    "    # We can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n",
    "    model = RandomForestClassifier(max_depth=max_depth, random_state=RANDOM_STATE).fit(\n",
    "        X_train, y_train\n",
    "    )\n",
    "    predictions_train = model.predict(X_train)\n",
    "    predictions_val = model.predict(X_val)\n",
    "    accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "    accuracy_val = accuracy_score(y_val, predictions_val)\n",
    "    accuracy_list_train.append(accuracy_train)\n",
    "    accuracy_list_val.append(accuracy_val)\n",
    "\n",
    "plt.title(\"Train x Validation metrics\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xticks(ticks=range(len(max_depth_list)), labels=max_depth_list)\n",
    "plt.plot(accuracy_list_train)\n",
    "plt.plot(accuracy_list_val)\n",
    "plt.legend([\"Train\", \"Validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd6d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_train = []\n",
    "accuracy_list_val = []\n",
    "for n_estimators in n_estimators_list:\n",
    "    # We can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators, random_state=RANDOM_STATE\n",
    "    ).fit(X_train, y_train)\n",
    "    predictions_train = model.predict(X_train)\n",
    "    predictions_val = model.predict(X_val)\n",
    "    accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "    accuracy_val = accuracy_score(y_val, predictions_val)\n",
    "    accuracy_list_train.append(accuracy_train)\n",
    "    accuracy_list_val.append(accuracy_val)\n",
    "\n",
    "plt.title(\"Train x Validation metrics\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xticks(ticks=range(len(n_estimators_list)), labels=n_estimators_list)\n",
    "plt.plot(accuracy_list_train)\n",
    "plt.plot(accuracy_list_val)\n",
    "plt.legend([\"Train\", \"Validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a943506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's then fit a random forest with the following parameters:\n",
    "\n",
    "# max_depth: 16\n",
    "# min_samples_split: 10\n",
    "# n_estimators: 100\n",
    "random_forest_model = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=16, min_samples_split=10\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(y_train, random_forest_model.predict(X_train)):.4f}\\nMetrics test:\\n\\tAccuracy score: {accuracy_score(y_val, random_forest_model.predict(X_val)):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b810e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "n = int(len(X_train) * 0.8)\n",
    "X_train_fit, X_train_eval, y_train_fit, y_train_eval = (\n",
    "    X_train[:n],\n",
    "    X_train[n:],\n",
    "    y_train[:n],\n",
    "    y_train[n:],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e34e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.1,\n",
    "    verbosity=1,\n",
    "    early_stopping_rounds=10,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "xgb_model.fit(\n",
    "    X_train_fit,\n",
    "    y_train_fit,\n",
    "    eval_set=[(X_train_eval, y_train_eval)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8284cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(y_train, xgb_model.predict(X_train)):.4f}\\nMetrics test:\\n\\tAccuracy score: {accuracy_score(y_val, xgb_model.predict(X_val)):.4f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-specialization-playground-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
