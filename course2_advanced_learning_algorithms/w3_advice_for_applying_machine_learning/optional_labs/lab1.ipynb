{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19aef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "# custom functions being developed interactively\n",
    "%aimport utils_practice_version\n",
    "import utils_practice_version as utils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# for building linear regression models and preparing data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# for building and training neural networks\n",
    "import tensorflow as tf\n",
    "\n",
    "# reduce display precision on numpy arrays\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# suppress warnings\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.keras.utils.disable_interactive_logging()\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 - Regression\n",
    "# Load the dataset from the csv file\n",
    "data = np.loadtxt(\"./data/data_w3_ex1.csv\", delimiter=\",\")\n",
    "\n",
    "# Split the inputs and outputs into separate arrays\n",
    "x = data[:, 0]\n",
    "y = data[:, 1]\n",
    "\n",
    "# Convert 1-D arrays into 2-D because the commands later will require it\n",
    "x = np.expand_dims(x, axis=1)\n",
    "y = np.expand_dims(y, axis=1)\n",
    "\n",
    "print(f\"The shape of the input x is : {x.shape}\")\n",
    "print(f\"The shape of the output y is : {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00921980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the entire dataset\n",
    "utils.plot_dataset(x, y, title=\"input vs. target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad1014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, cross validation, and test sets\n",
    "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.\n",
    "x_train, x_, y_train, y_ = train_test_split(\n",
    "    x,\n",
    "    y,\n",
    "    test_size=0.40,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "# Delete temporary variables\n",
    "del x_, y_\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_train_cv_test(\n",
    "    x_train, y_train, x_cv, y_cv, x_test, y_test, title=\"input vs. target\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4641cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Linear Model\n",
    "# Feature Scaling\n",
    "# Initialize the class\n",
    "scaler_linear = StandardScaler()\n",
    "\n",
    "# Compute the mean and standard deviation of the training set then transform it\n",
    "X_train_scaled = scaler_linear.fit_transform(x_train)\n",
    "\n",
    "print(f\"Computed mean of the training set: {scaler_linear.mean_.squeeze():.2f}\")\n",
    "print(\n",
    "    f\"Computed standard deviation of the training set: {scaler_linear.scale_.squeeze():.2f}\"\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "utils.plot_dataset(x=X_train_scaled, y=y_train, title=\"scaled input vs. target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Initialize the class\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "linear_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf31a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# Feed the scaled training set and get the predictions\n",
    "yhat = linear_model.predict(X_train_scaled)\n",
    "\n",
    "# Use scikit-learn's utility function and divide by 2\n",
    "print(f\"training MSE (using sklearn function): {mean_squared_error(y_train, yhat) / 2}\")\n",
    "\n",
    "mse = np.sum((yhat - y_train) ** 2) / (2 * len(yhat))\n",
    "print(f\"training MSE (manually calculated): {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the cross validation set using the mean and standard deviation of the training set\n",
    "X_cv_scaled = scaler_linear.transform(x_cv)\n",
    "\n",
    "print(f\"Mean used to scale the CV set: {scaler_linear.mean_.squeeze():.2f}\")\n",
    "print(\n",
    "    f\"Standard deviation used to scale the CV set: {scaler_linear.scale_.squeeze():.2f}\"\n",
    ")\n",
    "\n",
    "# Feed the scaled cross validation set\n",
    "yhat = linear_model.predict(X_cv_scaled)\n",
    "\n",
    "# Use scikit-learn's utility function and divide by 2\n",
    "print(f\"Cross validation MSE: {mean_squared_error(y_cv, yhat) / 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea46514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Polynomial Features\n",
    "# Create the additional features\n",
    "# Instantiate the class to make polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# Compute the number of features and transform the training set\n",
    "X_train_mapped = poly.fit_transform(x_train)\n",
    "\n",
    "# Preview the first 5 elements of the new training set. Left column is `x` and right column is `x^2`\n",
    "print(X_train_mapped[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd86e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the inputs as before.\n",
    "# Instantiate the class\n",
    "scaler_poly = StandardScaler()\n",
    "\n",
    "# Compute the mean and standard deviation of the training set then transform it\n",
    "X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "\n",
    "# Preview the first 5 elements of the scaled training set.\n",
    "print(X_train_mapped_scaled[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e95a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the class\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_mapped_scaled, y_train)\n",
    "\n",
    "# Compute the training MSE\n",
    "yhat = model.predict(X_train_mapped_scaled)\n",
    "print(f\"Training MSE: {mean_squared_error(y_train, yhat) / 2}\")\n",
    "\n",
    "# Add the polynomial features to the cross validation set\n",
    "X_cv_mapped = poly.transform(x_cv)\n",
    "\n",
    "# Scale the cross validation set using the mean and standard deviation of the training set\n",
    "X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "\n",
    "# Compute the cross validation MSE\n",
    "yhat = model.predict(X_cv_mapped_scaled)\n",
    "print(f\"Cross Validation MSE: {mean_squared_error(y_cv, yhat) / 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ddf9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to save the errors, models, and feature transforms\n",
    "train_mses = []\n",
    "cv_mses = []\n",
    "models = []\n",
    "polys = []\n",
    "scalers = []\n",
    "\n",
    "# Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "for degree in range(1, 11):\n",
    "\n",
    "    # Add polynomial features to the training set\n",
    "    poly = PolynomialFeatures(degree, include_bias=False)\n",
    "    X_train_mapped = poly.fit_transform(x_train)\n",
    "    polys.append(poly)\n",
    "\n",
    "    # Scale the training set\n",
    "    scaler_poly = StandardScaler()\n",
    "    X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "    scalers.append(scaler_poly)\n",
    "\n",
    "    # Create and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_mapped_scaled, y_train)\n",
    "    models.append(model)\n",
    "\n",
    "    # Compute the training MSE\n",
    "    yhat = model.predict(X_train_mapped_scaled)\n",
    "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "    train_mses.append(train_mse)\n",
    "\n",
    "    # Add polynomial features and scale the cross validation set\n",
    "    X_cv_mapped = poly.transform(x_cv)\n",
    "    X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "\n",
    "    # Compute the cross validation MSE\n",
    "    yhat = model.predict(X_cv_mapped_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "    cv_mses.append(cv_mse)\n",
    "\n",
    "# Plot the results\n",
    "degrees = range(1, 11)\n",
    "utils.plot_train_cv_mses(\n",
    "    degrees, train_mses, cv_mses, title=\"degree of polynomial vs. train and CV MSEs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2198ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the best model\n",
    "# Get the model with the lowest CV MSE (add 1 because list indices start at 0)\n",
    "# This also corresponds to the degree of the polynomial added\n",
    "degree = np.argmin(cv_mses) + 1\n",
    "print(f\"Lowest CV MSE is found in the model with degree={degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polynomial features to the test set\n",
    "X_test_mapped = polys[degree - 1].transform(x_test)\n",
    "\n",
    "# Scale the test set\n",
    "X_test_mapped_scaled = scalers[degree - 1].transform(X_test_mapped)\n",
    "\n",
    "# Compute the test MSE\n",
    "yhat = models[degree - 1].predict(X_test_mapped_scaled)\n",
    "test_mse = mean_squared_error(y_test, yhat) / 2\n",
    "\n",
    "print(f\"Training MSE: {train_mses[degree - 1]:.2f}\")\n",
    "print(f\"Cross Validation MSE: {cv_mses[degree - 1]:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2411687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 - Neural Networks\n",
    "# The same model selection process can also be used when choosing between different neural network architectures.\n",
    "# Step 1 - Prepare the data\n",
    "# You will use the same training, cross validation, and test sets you generated in the previous section. From earlier lectures in this course, you may have known that neural networks can learn non-linear relationships so you can opt to skip adding polynomial features. The code is still included below in case you want to try later and see what effect it will have on your results. The default degree is set to 1 to indicate that it will just use x_train, x_cv, and x_test as is (i.e. without any additional polynomial features).\n",
    "# Add polynomial features\n",
    "degree = 1\n",
    "poly = PolynomialFeatures(degree, include_bias=False)\n",
    "X_train_mapped = poly.fit_transform(x_train)\n",
    "X_cv_mapped = poly.transform(x_cv)\n",
    "X_test_mapped = poly.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbbe0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using the z-score\n",
    "scaler = StandardScaler()\n",
    "X_train_mapped_scaled = scaler.fit_transform(X_train_mapped)\n",
    "X_cv_mapped_scaled = scaler.transform(X_cv_mapped)\n",
    "X_test_mapped_scaled = scaler.transform(X_test_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6133213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize lists that will contain the errors for each model\n",
    "# nn_train_mses = {\"model_1\": [], \"model_2\": [], \"model_3\": []}\n",
    "# nn_cv_mses = {\"model_1\": [], \"model_2\": [], \"model_3\": []}\n",
    "# histories = {42: [], 1: [], 123: [], 99: [], 2026: [], 20: []}\n",
    "\n",
    "# seeds = [42, 1, 123, 99, 2026, 20]  # 6 different starting points\n",
    "# alpha = 0.1\n",
    "# epochs = 300\n",
    "# for seed in seeds:\n",
    "#     tf.random.set_seed(seed)\n",
    "\n",
    "#     # Build the models\n",
    "#     nn_models = utils.build_models()\n",
    "#     print(\"\\n----------------------------------------------------------\")\n",
    "#     print(f\"SEED = {seed}, Learning Rate = {alpha}, epochs = {epochs}\")\n",
    "#     print(\"----------------------------------------------------------\")\n",
    "\n",
    "#     # Loop over the the models\n",
    "#     for i, model in enumerate(nn_models):\n",
    "\n",
    "#         # Setup the loss and optimizer\n",
    "#         model.compile(\n",
    "#             loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=alpha)\n",
    "#         )\n",
    "\n",
    "#         # print(f\"Training {model.name}...\")\n",
    "\n",
    "#         # Train the model\n",
    "#         history = model.fit(X_train_mapped_scaled, y_train, epochs=epochs, verbose=0)\n",
    "#         histories[seed].append(history)\n",
    "\n",
    "#         # print(\"Done!\\n\")\n",
    "\n",
    "#         # Record the training MSEs\n",
    "#         yhat = model.predict(X_train_mapped_scaled)\n",
    "#         train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "#         nn_train_mses[model.name].append(train_mse)\n",
    "\n",
    "#         # Record the cross validation MSEs\n",
    "#         yhat = model.predict(X_cv_mapped_scaled)\n",
    "#         cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "#         nn_cv_mses[model.name].append(cv_mse)\n",
    "\n",
    "#     # print results\n",
    "#     print(\"RESULTS:\")\n",
    "#     for model in nn_models:\n",
    "#         print(\n",
    "#             f\"Model {model.name}: Training MSE: {nn_train_mses[model.name][-1]:.2f}, CV MSE: {nn_cv_mses[model.name][-1]:.2f}\"\n",
    "#         )\n",
    "# # Analyze results\n",
    "# print(\"\\n------------------------Averages----------------------------------\")\n",
    "# for name, errors in nn_train_mses.items():\n",
    "#     mean_err = np.mean(errors)\n",
    "#     std_err = np.std(errors)\n",
    "#     print(f\"{name}: Mean Training MSE = {mean_err:.2f} (+/- {std_err:.2f})\")\n",
    "\n",
    "# for name, errors in nn_cv_mses.items():\n",
    "#     mean_err = np.mean(errors)\n",
    "#     std_err = np.std(errors)\n",
    "#     print(f\"{name}: Mean CV MSE = {mean_err:.2f} (+/- {std_err:.2f})\")\n",
    "\n",
    "# utils.plot_nn_loss_curves(histories, alpha, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_mses = []\n",
    "nn_cv_mses = []\n",
    "\n",
    "# Build the models\n",
    "nn_models = utils.build_models2(20)\n",
    "\n",
    "# Loop over the the models\n",
    "for model in nn_models:\n",
    "\n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    )\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_mapped_scaled, y_train, epochs=300, verbose=0)\n",
    "\n",
    "    print(\"Done!\\n\")\n",
    "\n",
    "    # Record the training MSEs\n",
    "    yhat = model.predict(X_train_mapped_scaled)\n",
    "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "    nn_train_mses.append(train_mse)\n",
    "\n",
    "    # Record the cross validation MSEs\n",
    "    yhat = model.predict(X_cv_mapped_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "    nn_cv_mses.append(cv_mse)\n",
    "\n",
    "\n",
    "# print results\n",
    "print(\"RESULTS:\")\n",
    "for model_num in range(len(nn_train_mses)):\n",
    "    print(\n",
    "        f\"Model {model_num+1}: Training MSE: {nn_train_mses[model_num]:.2f}, \"\n",
    "        + f\"CV MSE: {nn_cv_mses[model_num]:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abc067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest CV MSE\n",
    "model_num = 3\n",
    "\n",
    "# Compute the test MSE\n",
    "yhat = nn_models[model_num - 1].predict(X_test_mapped_scaled)\n",
    "test_mse = mean_squared_error(y_test, yhat) / 2\n",
    "\n",
    "print(f\"Selected Model: {model_num}\")\n",
    "print(f\"Training MSE: {nn_train_mses[model_num-1]:.2f}\")\n",
    "print(f\"Cross Validation MSE: {nn_cv_mses[model_num-1]:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e84dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation for a classification task\n",
    "# Step 1 - Load the dataset\n",
    "data = np.loadtxt(\"./data/data_w3_ex2.csv\", delimiter=\",\")\n",
    "\n",
    "# Split the inputs and outputs into separate arrays\n",
    "x_bc, y_bc = data[:, :-1], data[:, -1]\n",
    "\n",
    "# Convert y into 2-D because the commands later will require it (x is already 2-D)\n",
    "y_bc = np.expand_dims(y_bc, axis=1)\n",
    "\n",
    "print(f\"the shape of the inputs x is: {x_bc.shape}\")\n",
    "print(f\"the shape of the targets y is: {y_bc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d921323",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_bc_dataset(x=x_bc, y=y_bc, title=\"x1 vs. x2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479842b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Preprocessing - Split the dataset\n",
    "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
    "x_bc_train, x_, y_bc_train, y_ = train_test_split(\n",
    "    x_bc, y_bc, test_size=0.40, random_state=1\n",
    ")\n",
    "\n",
    "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "x_bc_cv, x_bc_test, y_bc_cv, y_bc_test = train_test_split(\n",
    "    x_, y_, test_size=0.50, random_state=1\n",
    ")\n",
    "\n",
    "# Delete temporary variables\n",
    "del x_, y_\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_bc_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_bc_train.shape}\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_bc_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_bc_cv.shape}\")\n",
    "print(f\"the shape of the test set (input) is: {x_bc_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_bc_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e661ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing - Scale the features\n",
    "# Initialize the class\n",
    "scaler_linear = StandardScaler()\n",
    "\n",
    "# Compute the mean and standard deviation of the training set then transform it\n",
    "x_bc_train_scaled = scaler_linear.fit_transform(x_bc_train)\n",
    "x_bc_cv_scaled = scaler_linear.transform(x_bc_cv)\n",
    "x_bc_test_scaled = scaler_linear.transform(x_bc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618955aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding classification model performance evaluation through an example\n",
    "# Get performance metric by getting the fraction of the data that the model has misclassified.\n",
    "# Sample model output\n",
    "probabilities = np.array([0.2, 0.6, 0.7, 0.3, 0.8])\n",
    "\n",
    "# Apply a threshold to the model output. If greater than 0.5, set to 1. Else 0.\n",
    "predictions = np.where(probabilities >= 0.5, 1, 0)\n",
    "\n",
    "# Ground truth labels\n",
    "ground_truth = np.array([1, 1, 1, 1, 1])\n",
    "\n",
    "print(f\"probabilities: {probabilities}\")\n",
    "print(f\"predictions with threshold=0.5: {predictions}\")\n",
    "print(f\"targets: {ground_truth}\")\n",
    "print(f\"fraction of misclassified data : {np.mean(predictions != ground_truth)}\")\n",
    "print(\n",
    "    f\"fraction of misclassified data without thresholding: {np.mean((probabilities >= 0.5) != ground_truth)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bfd14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Build and train the model\n",
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_error = []\n",
    "nn_cv_error = []\n",
    "\n",
    "# Build the models\n",
    "models_bc = utils.build_models2(random_state=20)\n",
    "\n",
    "# Loop over each model\n",
    "for model in models_bc:\n",
    "\n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    )\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_bc_train_scaled, y_bc_train, epochs=200, verbose=0)\n",
    "\n",
    "    print(\"Done!\\n\")\n",
    "\n",
    "    # Set the threshold for classification\n",
    "    threshold = 0.5\n",
    "\n",
    "    # Record the fraction of misclassified examples for the training set\n",
    "    yhat = tf.math.sigmoid(model.predict(x_bc_train_scaled))\n",
    "    train_error = np.mean((yhat >= threshold) != y_bc_train)\n",
    "    nn_train_error.append(train_error)\n",
    "\n",
    "    # Record the fraction of misclassified examples for the cross validation set\n",
    "    yhat = tf.math.sigmoid(model.predict(x_bc_cv_scaled))\n",
    "    cv_error = np.mean((yhat >= threshold) != y_bc_cv)\n",
    "    nn_cv_error.append(cv_error)\n",
    "\n",
    "# Print the result\n",
    "for model_num in range(len(nn_train_error)):\n",
    "    print(\n",
    "        f\"Model {model_num+1}: Training Set Classification Error: {nn_train_error[model_num]:.5f}, \"\n",
    "        + f\"CV Set Classification Error: {nn_cv_error[model_num]:.5f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20eded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest error\n",
    "model_num = 3\n",
    "\n",
    "# Compute the test error\n",
    "yhat = models_bc[model_num - 1].predict(x_bc_test_scaled)\n",
    "yhat = tf.math.sigmoid(yhat)\n",
    "nn_test_error = np.mean((yhat >= threshold) != y_bc_test)\n",
    "\n",
    "print(f\"Selected Model: {model_num}\")\n",
    "print(f\"Training Set Classification Error: {nn_train_error[model_num-1]:.4f}\")\n",
    "print(f\"CV Set Classification Error: {nn_cv_error[model_num-1]:.4f}\")\n",
    "print(f\"Test Set Classification Error: {nn_test_error:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-specialization-playground-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
